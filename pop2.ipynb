{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import io\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from collections import defaultdict\n",
    "rankings = defaultdict(dict)\n",
    "import operator\n",
    "# making  necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Papers:                                   # Class for paper\n",
    "    def __init__(self,contents,author,index,is_expert):\n",
    "        self.author = author\n",
    "        self.index = index\n",
    "        self.expert = is_expert\n",
    "        self.contents =contents\n",
    "    def assignindex (self,index):\n",
    "        self.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts:                               # Class for Expert \n",
    "    def __init__(self,paper,name):\n",
    "        self.name = name\n",
    "        self.papers = [paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfid(contents):                 # function to return Tfidf matrix of all papers         \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.93, max_features=20000,\n",
    "                                     min_df=0.05, stop_words='english',lowercase = True,\n",
    "                                     use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(contents) #fit the vectorizer to contents of papers\n",
    "    print(tfidf_matrix.shape)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans (n_clusters,tfidf_matrix):                 # clusters Tfidf matrix\n",
    "    km = KMeans(n_clusters=n_clusters)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfun(sim,expert,items):\n",
    "    a = [sim[items[0],paper.index]for paper in expert.papers]\n",
    "    if(a ==[]):\n",
    "        return 0\n",
    "    else:\n",
    "        return max(a)\n",
    "def semi_supervisedLearning(lables,sim,no_of_clusters,Experts):\n",
    "    i=0\n",
    "    indexed_lables = []\n",
    "    for l in lables:\n",
    "        indexed_lables.append((i,l))\n",
    "        i=i+1\n",
    "    for cluster in range(0,no_of_clusters):\n",
    "        for expert in Experts:\n",
    "\n",
    "            rankings[cluster][expert.name] = 0\n",
    "    for cluster in range(0,no_of_clusters):\n",
    "        for expert in Experts:\n",
    "            for items in list(filter((lambda x : x[1] == cluster),indexed_lables)) :\n",
    "                    rankings[cluster][expert.name] = rankings[cluster][expert.name] + (getfun(sim,expert,items))\n",
    "    return rankings\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('evaluation/short_listed_revs.txt','r')                #contains  expert names\n",
    "expert_names =file.readlines()                                  \n",
    "reviewers_dict ={}                                   # a dictionary of expert object indexed by names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in expert_names:                                 \n",
    "    author=(author.strip())                        # creating expert objects and adding then to dictionary\n",
    "    with io.open('title_abstract_data/'+ author +'.txt','r',encoding='utf-8',errors='ignore') as infile, \\\n",
    "    io.open('temp.txt','w',encoding='utf-8',errors='ignore') as outfile:\n",
    "        for line in infile:\n",
    "            print(*line.split(), file=outfile)\n",
    "    file1 = open(\"temp.txt\",\"r\")\n",
    "    for eachline in file1.readlines():\n",
    "        p1 = Papers(eachline,author,-1,True)\n",
    "        reviewer = reviewers_dict.get(author)\n",
    "        if(reviewer != None):\n",
    "            reviewer.papers.append(p1)\n",
    "        else:\n",
    "            e1 = Experts(p1,author)\n",
    "            reviewers_dict[author] = e1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"evaluation/titles.txt\", \"r\")               # now for papers for which we neeed recommendation\n",
    "lines =file.readlines()\n",
    "\n",
    "with io.open('evaluation/abstracts.txt','r',encoding='utf-8',errors='ignore') as infile, \\\n",
    "     io.open('temp.txt','w',encoding='utf-8',errors='ignore') as outfile:\n",
    "    for line in infile:\n",
    "        print(*line.split(), file=outfile)\n",
    "file1 = open(\"temp.txt\",\"r\")\n",
    "contents = file1.readlines()\n",
    "research_papers =[]    #list of paper objects for which we need recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, c in zip(lines, contents):        #adding to research paper list\n",
    "    l=l.strip()\n",
    "    content = str(l) + str(contents)\n",
    "    p1 = Papers(content,-1,-1,False)\n",
    "    research_papers.append(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "expertlist = list(reviewers_dict.values())       # assigning unique index to each paper\n",
    "all_paper=[]\n",
    "all_paper_objects=[]\n",
    "for expert in expertlist:\n",
    "    for paper in expert.papers:\n",
    "        paper.assignindex(i)\n",
    "        all_paper.append(paper.contents)\n",
    "        all_paper_objects.append(paper)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in research_papers:\n",
    "        paper.assignindex(i)\n",
    "        all_paper.append(paper.contents)\n",
    "        all_paper_objects.append(paper)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20754\n",
      "20754\n"
     ]
    }
   ],
   "source": [
    "print (len(all_paper))\n",
    "print (len(all_paper_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20754, 125)\n"
     ]
    }
   ],
   "source": [
    "tfidf_mat = get_tfid(all_paper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity=(cosine_similarity(tfidf_mat));       # generating similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Kmeans(3,tfidf_mat)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = semi_supervisedLearning(labels,similarity,3,expertlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1_recommend = sorted(scores[0].items(), key=operator.itemgetter(1) , reverse = True)\n",
    "cl2_recommend = sorted(scores[1].items(), key=operator.itemgetter(1), reverse = True)\n",
    "cl3_recommend = sorted(scores[2].items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (cl1_recommend[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (cl2_recommend[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (cl3_recommend[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
